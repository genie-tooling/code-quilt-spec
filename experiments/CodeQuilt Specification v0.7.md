CodeQuilt Specification v0.7.2-LLM-Semantic

Version: 0.7.2-semantic-py3.11-std25-v1 (Example Version String)
Date: 2025-04-07 (Assumed date)
Status: Draft
Changes from v0.7.1:

    Added optional O:[tok_mode=verbose] for alternative tokenization.

    Added optional, simpler semantic tokens as alternatives/components (e.g., LOGERR).

    Added clarifications on parameter parsing and LLM guidance.

    Ensured backward compatibility: Decoders supporting v0.7.2 MUST correctly decode valid v0.7.1 streams.

Goal: Achieve high token compression for LLM interaction with Python code by combining dense syntactic tokens, an implied standard corpus dictionary, a dynamic dictionary, and LLM-generatable semantic pattern tokens. Aims for efficiency gains noticeable below 25k original tokens, with considerations for smoother LLM processing.

1. Introduction & Purpose

CodeQuilt v0.7.2-LLM-Semantic is designed for representing source code (primarily Python) in a highly compressed, text-based format suitable for Large Language Models (LLMs). It acts as a robust blueprint, minimizing token exchange during generation, debugging, and modification tasks.

This version integrates several compression strategies:

    Dense Fixed Tokens: Single ASCII characters for core syntax (keywords, operators, structure) - Default Mode.

    Implied Corpus Dictionary (C): A standardized dictionary, defined by the spec version string (V:), mapping c<n> references to common Python built-ins, stdlib elements, and literals. This dictionary is not transmitted in the header.

    Dynamic Identifier Dictionary (D): A per-message header field (D:) mapping d<n> references to identifiers not present in the implied Corpus Dictionary.

    Semantic Pattern Tokens: Compact tokens representing common logical patterns (e.g., error handling, checks, common assignments), designed to be recognized and generated by the LLM itself. Includes preferred complex patterns and optional simpler alternatives.

    Efficient Literal Handling: Inline short literals, X: header field with Base64 for long/multiline literals (l<n>).

    Comment Removal: Comments are discarded by default (O:cmt=k to preserve).

    (New in 0.7.2) Optional Verbose Token Mode: An option (O:tok_mode=verbose) allows using standard Python keywords/operators instead of dense fixed tokens, potentially improving compatibility with some LLM tokenizers at the cost of compression.

Human readability is sacrificed for token density in the default mode.

2. Overall Format

A CodeQuilt string consists of two parts separated by the delimiter |||:

[Header]|||<Body>

    Header: Contains metadata (V:, D:, etc.). The Corpus Dictionary is implied by V:. Enclosed in square brackets [].

    Body: The compressed sequence of tokens representing the code logic and structure.

Formally (EBNF):
codequilt-string ::= "[" header "]" "|||" body-tokens

3. Header Specification ([Header])

Uses key-value pairs separated by semicolons ;. Keys are short, case-sensitive. No unnecessary whitespace.

Formally (EBNF):
header ::= "[" header-field (";" header-field)* "]"

3.1. Header Fields (v0.7.2-semantic)

    V:<version>: (Required) CodeQuilt specification version string. This string uniquely identifies the spec version AND the specific Implied Corpus Dictionary (C) and Fixed Token Mappings (L:) being used.

        Example: V:0.7.2-semantic-py3.11-std25-v1

    L:<language>[-version]: (Implied by V:, Optional Override) The target language. Generally redundant if V: is specific, but can be included for clarity.

        Example: L:python-3.11

    D:[d<n>=identifier,...]: (Required if non-corpus identifiers exist) Dynamic Identifier Dictionary. Maps d<n> IDs (sequential, starting d0) to identifiers NOT found in the Implied Corpus Dictionary (C). Omit or use D:[] if all identifiers are corpus-mapped. identifier follows Python identifier rules.

        Example: D:[d0=my_custom_var,d1=process_specific_data]

    I:[path1,path2 as alias,...]: (Optional) List of modules/libraries imported (informational).

        Example: I:[os,pandas as pd]

    O:[opt1,opt2=value,...]: (Optional) Reconstruction options/flags.

        Defined Options:

            cmt=k: (Keep Comments) If present, comments MUST be preserved via Base64 encoding in the X: dictionary, referenced by l<n>. Default: Comments Discarded.

            lth=N: (Literal Threshold) Max character length N for string/bytes literals embedded directly in the body. Default: 80 (Recommendation).

            tok_mode=verbose: (New in 0.7.2) If present, the body uses standard Python keywords (e.g., def, if, and) and operators (e.g., ==, !=, +=) instead of the default dense single-character fixed tokens. Structure tokens (N > < ( ) [ ] { } , : . ;) and References (c/d/l<n>) remain the same. Default: Dense fixed tokens.

        Example: O:[lth=120,tok_mode=verbose]

    C:<algo>-<hash>: (Optional) Checksum of the intended final code.

        Example: C:sha256-a1b2c3d4...

    X:[l<n>=base64(literal),...]]: (Optional) Extended Literal Dictionary. Maps l<n> IDs (sequential, starting l0) to Base64 encoded literals (UTF-8 assumed) that are multiline, exceed lth, or are comments (if O:cmt=k).

        Example: X:[l0=U0VMRUNUI...==,l1=IyBMb25nIGNvbW1lbnQ...]

3.2. Implied Corpus Dictionary (C)

    This dictionary (c<n>=name_or_literal) is NOT included in the header string.

    Its content is fixed and defined by the V: version string. (See Section 5 for V:0.7.2... which is identical to V:0.7.1... for this field).

    Both the generator (LLM via prompt) and the reconstructor MUST use the standard Corpus Dictionary corresponding to the specified V:.

4. Body Specification (<Body>)

A continuous string of tokens. Whitespace characters between tokens are ignored, except within inline string literals.

Formally (EBNF - Simplified):
body-tokens ::= (token)*
token ::= structure-token | fixed-token | semantic-token | corpus-ref | dynamic-ref | literal-ref | literal-token | preprocessor-token | escape-hatch-token | decorator-prefix

4.1. Token Types

    Structure Tokens (structure-token): Always single characters.

        N: Newline.

        >: Increase indentation level for the next line.

        <: Decrease indentation level for the next line.

        ( ) [ ] { } , : . ;: Standard delimiters.

    Fixed Tokens (fixed-token): Representation depends on O:tok_mode.

        Default Mode (tok_mode not verbose): Single ASCII characters defined by the language profile (V:/L:). See Section 6. Examples (Python): D(def), C(class), ?(if), +, -, =, &(and), |(or), t(True).

        Verbose Mode (tok_mode=verbose): Standard Python keywords (e.g., def, if, and, or, True, None) and operators (e.g., +, -, ==, !=, +=). Multi-character operators are treated as single tokens in this mode.

    Semantic Tokens (semantic-token): Compact representations of common logical patterns. Always start with an uppercase identifier. See Section 7.

        Examples: LOG(...), ATTR(...), TRYLOG(...), LOGERR(...) (New optional).

    Corpus Reference (corpus-ref):

        Format: c<n> (e.g., c0, c25). Refers to entry n in the Implied Corpus Dictionary.

    Dynamic Reference (dynamic-ref):

        Format: d<n> (e.g., d0, d1). Refers to entry n in the Dynamic Dictionary (D:).

    Literal Reference (literal-ref):

        Format: l<n> (e.g., l0, l1). Refers to the Base64-decoded entry n in the Extended Literal Dictionary (X:).

    Literal Tokens (literal-token): Direct embedding of simple values.

        Numbers: 123, 3.14, -10.

        Booleans (Default Mode): t (True), f (False). (In Verbose Mode, use True, False).

        None (Default Mode): n (None). (In Verbose Mode, use None).

        Short Strings: 'abc', "def\\'s" (Single-line, length <= lth, uses host language escapes like \\, \', \n).

        Short Bytes: b'xyz', b"\\x01" (Single-line, length <= lth).

    Preprocessor Tokens (preprocessor-token): Language-specific (e.g., C/C++ #ifdef). Defined by L:. (Not detailed for Python here).

    Escape Hatch (escape-hatch-token): L'{...}'. Last resort. Content escapes \\->\\\\, }->\\}, {->\\{.

    Decorator Prefix (decorator-prefix): @. Language-specific (Python). Applies to the immediately following token (usually a ref or fixed token representing the decorator function).

Note on Comments: Discarded by default. Use O:cmt=k to preserve via X:/l<n>. There is no explicit comment token in the body.

Note on Parameter Separator (:): Within Semantic Tokens, the colon : separates parameters. It is not treated as a separator if it appears inside a correctly quoted inline string or bytes literal being used as a parameter.

5. Standard Corpus Dictionary (C) - V:0.7.2-semantic-py3.11-std25-v1

    Identical to v0.7.1. This dictionary is fixed and published for the specific V: string.

    Derived from static analysis of a representative corpus (e.g., Python stdlib + top N packages).

    The LLM (via prompt) and Reconstructor must use the correct version.

(The full list c0-c251 provided in the v0.7.1 spec applies here without change).

6. Fixed Token Mappings (Python Example - Default Mode)

Implied by V:/L:. Defines single-character tokens used when O:tok_mode=verbose is not set.

    Structure: N > < ( ) [ ] { } , : . ;

    Operators (Primary): = + - * / % < > .

    Operators (Mapped):

        &: and

        |: or

        ^: ^ (xor) (Note: ** becomes * *)

        ~: is

        !: not (Logical not, potentially ambiguous with !=. Need context or use ! = for !=) -> Refinement: Let ! be not. != must be the sequence ! =. Similarly == is = =, >= is > =, <= is < =, += is + =, etc.

        @: in

    Keywords (Uppercase): D(def), C(class), R(return), ?(if), T(try), X(except), Y(finally), P(pass), M(import), J(from), E(elif), B(break), K(continue), Q(del), A(await), S(async), U(raise), Z(assert), W(with), G(global), L(lambda)

    Literals: t(True), f(False), n(None)

(Final mapping requires empirical LLM tokenizer testing. Use O:tok_mode=verbose to bypass if issues arise).

7. Semantic Pattern Tokens (Python Examples)

Designed for LLM generation/recognition. Parameters use c<n>, d<n>, l<n> or inline literals. : separates parameters. {} encloses body tokens for patterns like TRYLOG. Encoders should prefer complex tokens (like TRYLOG) but may use simpler alternatives (like LOGERR) if pattern matching is easier. Decoders MUST support all defined tokens.

(Assume c105=logger, c0=Exception, t=True, n=None, etc. from Corpus C for expansions)

    Preferred Complex Tokens:

        LOG(<lvl>:<fmt>[:<args...>]) -> c105.<lvl>(<fmt>, *<args>) [<lvl>: i, d, e, w, c (fixed tokens map to info, debug, etc.)]

        TRYLOG(<err>:<var> {<body>}) -> T : N > <body> N < X <err> as <var> : N > c105.e(f"FAIL: {<var>}", exc_info=t) N <

        DBEXEC(<sql>[:<params>]) -> (Standard try/with conn/execute/commit/except log/rollback/raise block)

        DBFETCH1(<tgt>:<sql>[:<params>]) -> (Standard try/with conn/execute/fetchone/assign/commit/except log/rollback/raise block)

        CHKEXIT(<msg>:<exit_code>) -> (Standard orchestrator pattern audit/report/raise SystemExit)

        CHKINIT(<var>:<Class>[:<args...>]) -> G <var> N ? <var> ~ n : N > <var> = <Class>(*<args>) N <

        PATHJOIN(<tgt>:<part1>[:<parts...>]) -> <tgt> = c115 . c78 (<part1>, *<parts>) (Assume c115=os.path/pathlib, c78=join)

        MKDIRS(<path>) -> c115 . c121 (<path>, exist_ok=t) (Assume c115=os/pathlib, c121=makedirs)

    Simple/Component Tokens (Often represent common single lines):

        RETN(<var>) -> ? <var> ~ n : N > R n N < (If var is None, return None)

        RETF(<var>) -> ? ! <var> : N > R n N < (If var is False/Empty, return None)

        RAISE(<chk>:<var>:<err>[:<args...>]) -> ? <check> : N > U <err>(*<args>) N < [<chk>: N (<var>~n), E (!<var>)].

        ATTR(<obj>:<attr>:<val>) -> <obj> . <attr> = <val>

        DGET(<tgt>:<dct>:<key>:<def>) -> <tgt> = <dct> . get ( <key> , <def> )

        LOGERR(<err_var>[:<msg_prefix>]) -> (New in 0.7.2) c105.e(f"<prefix>FAIL: {<err_var>}", exc_info=t) (Common logging line in except blocks. <msg_prefix> is optional).

(This list can be extended. Encoders should prioritize larger patterns.)

8. Examples

(Examples from v0.7.1 remain conceptually valid, assuming default token mode. If tok_mode=verbose was used, fixed tokens like D, R, & would be replaced by def, return, and etc.)

9. Reconstruction Notes

    Requires: The specific Implied Corpus Dictionary (C) and Fixed Token Mappings associated with the V: string.

    Process:

        Parse Header: Extract V:, D:, I:, O:, C:, X:. Determine tok_mode.

        Build dynamic lookup from D:. Decode Base64 values from X:.

        Load the correct standard Corpus Dictionary C based on V:.

        Tokenize Body: Recognize tokens based on the rules in Section 4, considering tok_mode. Handle references (c/d/l<n>), literals, semantic tokens, structure tokens, escape hatches, etc.

        Reconstruct Code:

            Substitute c<n>, d<n>, l<n> references.

            Translate fixed tokens based on tok_mode (single chars or verbose keywords/ops).

            Expand Semantic Tokens: Replace each semantic token (LOG, ATTR, TRYLOG, LOGERR, etc.) with its corresponding Python code block, substituting parameters correctly.

            Handle structure tokens (N, >, <) to format indentation and newlines.

            Apply spacing heuristics (less critical if using a formatter).

        Post-process (e.g., black formatter is recommended), verify checksum (C:).

10. Considerations for LLM Generation (v0.7.2)

    Prompting is Key: The LLM prompt MUST include:

        The full CodeQuilt v0.7.2 specification (or the reduced spec).

        The entire content of the specific Corpus Dictionary (C) implied by the target V: string.

        Clear definitions and syntax for all Semantic Tokens (preferred complex ones AND simpler alternatives).

        Instructions to map unknown identifiers to D: / d<n>. Prioritize using c<n> where possible.

        Instructions to use X: / l<n> for long/multiline literals and comments (if cmt=k).

        Explicit instruction to prefer the larger, complex Semantic Tokens (like TRYLOG, DBEXEC) where applicable, but allow falling back to simpler ones (like LOGERR) if the larger pattern isn't fully matched or is harder to generate.

        Guidance on tok_mode: Default is dense fixed tokens. Mention tok_mode=verbose as an option if compatibility issues are suspected, trading off compression.

        Reinforce the stateful nature of indentation (N > <).

    LLM Pattern Matching: Relies on the LLM's ability to recognize Python patterns corresponding to semantic tokens. Providing both complex and simpler alternatives gives the LLM more flexibility.

    Fidelity: Semantic tokens reconstruct functionally equivalent code, but potentially not character-identical code (e.g., exact whitespace within the pattern, slightly different logging format string construction). This is acceptable for the goal of compressed representation.

    Tokenizer Awareness: Be mindful that ultra-dense fixed tokens (default mode) might interact strangely with some LLM tokenizers. Testing is recommended. Use tok_mode=verbose as a fallback.

CodeQuilt v0.7.2-semantic-py3.11-std25-v1: LLM Encoder/Decoder Context (Reduced)

Goal: Highly compressed Python 3.11 text for LLMs. Min tokens via fixed chars (default) or verbose tokens (O:tok_mode=verbose), implied corpus (C), dynamic dict (D), extended lits (X), semantic patterns (S). Backward compat w/ v0.7.1 decode. This IS the spec & data.

Format: [Header]|||Body

Header ([...]): ;-sep Key:Value. Keys: V,L,D,I,O,C,X. Case-sens. No extra space.

    V:0.7.2-semantic-py3.11-std25-v1 (REQUIRED/FIXED). Implies Corpus C & Fixed Tokens below.

    L:python-3.11 (Optional, implied).

    D:[d<n>=id,...] (Dynamic): REQUIRED for non-C IDs. Start d0. d<n> -> id. Empty: D:[]/omit. id: Py identifier.

    X:[l<n>=b64,...] (Extended Lits): REQUIRED if lit > lth, multiline, or comment (cmt=k). Start l0. b64: Std Base64(UTF-8). l<n> -> decoded str.

    I:[path,...] (Imports): Optional info. path/alias: safe-hdr-char+ (a-zA-Z0-9_./+:-=).

    O:[opt1,...] (Options): Optional. Defined: cmt=k (keep comments via X:/l<n>), lth=N (inline lit max len, def 80), tok_mode=verbose (New: use Py keywords/ops like def, == instead of dense D, = =). val: safe-hdr-char+.

    C:<algo>-<hash>: Optional checksum (sha256-a1b...).

Body (... after |||): Token sequence. Whitespace chars BETWEEN tokens ignored.

Body Tokens:

    Structure: N(newline), >(indent+1), <(dedent-1), ( ) [ ] { } , : . ; (Always single chars).

    Fixed Toks (Default Mode OR tok_mode=verbose):

        Default: Single chars (See Map Below). E.g., D -> def, ? -> if, & -> and, t -> True. Multi-char ops are sequences (e.g., ! = for !=).

        Verbose (tok_mode=verbose): Standard Python. E.g., def, if, and, True, ==, !=, +=.

    Refs: c<n>(Corpus C idx n), d<n>(Header D idx n), l<n>(Header X idx n, b64-decoded).

    Inline Lits: 123, -10, 3.14. Default: t, f, n. Verbose: True, False, None. Strings/Bytes: 'abc', "d'ef", b'xy', b"\\x01". MUST be <= lth & single-line. Escapes: \\ \' \" \n \t etc. Colon : inside quoted literal is NOT param separator.

    Semantic Tok (See Defs Below): LOG(...), ATTR(...), TRYLOG(...), LOGERR(...) etc. Uppercase name, ( params ) or ( params : { body } ). Params sep by :. Prefer complex tokens (e.g. TRYLOG) but allow simpler alternatives (e.g. LOGERR).

    Decorator: @ (prefix).

    Escape: L'{raw_code}'. Raw escapes: \\ -> \\\\, } -> \\}, { -> \\{.

Corpus C (for V:0.7.2...; c<n> maps to entry n)
(Identical list c0-c251 as v0.7.1)
c0=Exception c1=AttributeError ... c251=request

Fixed Tokens (Default Mode - Implied by V)

    Struct: N > < ( ) [ ] { } , : . ;

    Ops: = + - * / % & | ^ ~ < > ! @ (&:and, |:or, ^:xor, ~:is, !:not, @:in). NOTE: Multi-char ops like ==, !=, += MUST be sequences (= =, ! =, + =).

    Kw: D def, C class, R return, ? if, T try, X except, Y finally, P pass, M import, J from, E elif, B break, K continue, Q del, A await, S async, U raise, Z assert, W with, G global, L lambda

    Lit: t True, f False, n None

    (Use O:tok_mode=verbose for standard Py tokens if needed)

Semantic Tokens (S) Definitions & Expansions (Prefer Complex)
(Assume c105=logger, c0=Exception, t=True, n=None etc.)

    Complex (Preferred):

        LOG(<lvl>:<fmt>[:<args...>]) -> c105.<lvl>(<fmt>, *<args>) [lvl: i/d/e/w/c]

        TRYLOG(<err>:<var> {<body>}) -> T : N > <body> N < X <err> as <var> : N > c105.e(f"FAIL: {<var>}", exc_info=t) N <

        DBEXEC(<sql>[:<params>]) -> (Std try/with conn/exec/commit/except log/rollback/raise)

        DBFETCH1(<tgt>:<sql>[:<params>]) -> (Std try/with conn/exec/fetchone/assign/commit/except log/rollback/raise)

        CHKEXIT(<msg>:<exit_code>) -> (_audit/_report/raise SystemExit)

        CHKINIT(<var>:<Class>[:<args...>]) -> G <var> N ? <var> ~ n : N > <var> = <Class>(*<args>) N <

        PATHJOIN(<tgt>:<part1>[:<parts...>]) -> <tgt> = c115 . c78 (<part1>, *<parts>) (c115=path, c78=join)

        MKDIRS(<path>) -> c115 . c121 (<path>, exist_ok=t) (c115=os/pathlib, c121=makedirs)

    Simple/Component (Alternatives):

        RETN(<var>) -> ? <var> ~ n : N > R n N <

        RETF(<var>) -> ? ! <var> : N > R n N <

        RAISE(<chk>:<var>:<err>[:<args...>]) -> ? <check> : N > U <err>(*<args>) N < [chk: N/E]

        ATTR(<obj>:<attr>:<val>) -> <obj> . <attr> = <val>

        DGET(<tgt>:<dct>:<key>:<def>) -> <tgt> = <dct> . get ( <key> , <def> )

        LOGERR(<err_var>[:<prefix>]) -> c105.e(f"<prefix>FAIL: {<err_var>}", exc_info=t) (New Optional)

Encoding/Decoding Logic Summary:

    Decode: Parse Header (get D/X/O -> det tok_mode) -> Load C + Fixed Toks -> Tokenize Body (respect tok_mode) -> Subst c/d/l -> Expand SemToks -> Translate Fixed Toks -> Format (N > <). Verify C: hash. Handles v0.7.1 input.

    Encode: Parse Code -> Prefer Complex SemTok patterns, use simple if needed -> Map Kw/Op to Fixed Toks (respect tok_mode) -> Map IDs/Lits to C (c<n>) or build D (d<n>) -> Handle Lits (Inline vs X:l<n>) -> Handle comments (cmt=k -> X:l<n>) -> Insert Structure (N > <) -> Build Header -> Assemble.